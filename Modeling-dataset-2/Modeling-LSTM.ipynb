{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeding_matrix = load_pickle_file(\"./embeding_matrix\")\n",
    "word2index = load_pickle_file(\"./word2index\")\n",
    "index_sequences = load_pickle_file(\"./index_sequences.pkl\")\n",
    "labels = load_pickle_file(\"./labels\")\n",
    "SEQUENCE_LENGTH = 618\n",
    "VOCAB_SIZE = len(embeding_matrix)\n",
    "EMB_DIMENSION = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "PAD_SEQ_VALUE = 3001066"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(embeding_matrix)\n",
    "PAD_SEQ_VALUE = 3001066\n",
    "labels = load_pickle_file(\"./labels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "same_lenght_seq = [seq[:SEQUENCE_LENGTH].reshape(1,SEQUENCE_LENGTH) if len(seq) >= SEQUENCE_LENGTH \n",
    "                   else np.concatenate((seq,np.array([PAD_SEQ_VALUE] * (SEQUENCE_LENGTH - len(seq)))),axis = 0).reshape(1,SEQUENCE_LENGTH) \n",
    "                   for seq in index_sequences]\n",
    "same_lenght_seq = np.concatenate(same_lenght_seq,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "test_size = int(len(labels) * 0.2)\n",
    "true_labels = np.where(np.array(labels)==0)[0]\n",
    "start_index = true_labels[0]\n",
    "random_test_true_indexes = [int(x * len(true_labels)+start_index) for x in np.random.random(test_size//2)]\n",
    "random_test_false_indexes = [int(x * (len(labels) - len(true_labels))) for x in np.random.random(test_size//2)]\n",
    "x_test_indexes = random_test_true_indexes + random_test_false_indexes\n",
    "random.shuffle(x_test_indexes)\n",
    "\n",
    "x_train_indexes = [x for x in range(0,len(labels)) if x not in x_test_indexes]\n",
    "\n",
    "random.shuffle(x_train_indexes)\n",
    "\n",
    "labels = np.vstack(np.array([1,0]).reshape(1,2) if x == 0 else np.array([0,1]).reshape(1,2) for x in labels)\n",
    "\n",
    "y_test = np.array(labels)[x_test_indexes]\n",
    "y_train = np.array(labels)[x_train_indexes]\n",
    "\n",
    "x_train = same_lenght_seq[x_train_indexes]\n",
    "x_test = same_lenght_seq[x_test_indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(same_lenght_seq, labels, test_size=0.2, random_state=42,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = len(x_train)\n",
    "perm = np.random.permutation(data_size)\n",
    "idx_train = perm[:int(data_size*(1-VALIDATION_SPLIT))]\n",
    "idx_val = perm[int(data_size*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "data_train = x_train[idx_train]\n",
    "labels_train = y_train[idx_train]\n",
    "\n",
    "data_val = x_train[idx_val]\n",
    "labels_val = y_train[idx_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7380, 434)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>LSTM</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras as tk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers.normalization import BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = ModelCheckpoint(\"./Checkpoints\", save_best_only=True, save_weights_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_HIDDEN_DIM_SIZE = 64\n",
    "rate_drop_lstm = 0.15\n",
    "rate_drop_lstm = 0.15\n",
    "lstm_model = tk.Sequential()\n",
    "embedding_layer = Embedding(VOCAB_SIZE,\n",
    "        EMB_DIMENSION,\n",
    "        weights=[embeding_matrix],\n",
    "        input_length=SEQUENCE_LENGTH,\n",
    "        trainable=False)\n",
    "lstm_model.add(embedding_layer)\n",
    "lstm_model.add(LSTM(LSTM_HIDDEN_DIM_SIZE, activation = 'tanh', dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm,return_sequences = True))\n",
    "lstm_model.add(LSTM(LSTM_HIDDEN_DIM_SIZE//2, activation = 'tanh', dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm))\n",
    "lstm_model.add(Dense(LSTM_HIDDEN_DIM_SIZE//4, activation = 'relu'))\n",
    "# lstm_model.add(LSTM(LSTM_HIDDEN_DIM_SIZE//4, activation = 'tanh',dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm))\n",
    "lstm_model.add(Dense(2,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "del lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 618, 300)          900320100 \n",
      "_________________________________________________________________\n",
      "unified_lstm_2 (UnifiedLSTM) (None, 618, 64)           93440     \n",
      "_________________________________________________________________\n",
      "unified_lstm_3 (UnifiedLSTM) (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 900,426,518\n",
      "Trainable params: 106,418\n",
      "Non-trainable params: 900,320,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tk.optimizers.Adam(learning_rate=0.007)\n",
    "# opt = tk.optimizers.RMSprop(\n",
    "#     learning_rate=0.002,\n",
    "#     rho=0.9,\n",
    "#     momentum=0.0,\n",
    "#     epsilon=1e-07,\n",
    "#     centered=False,\n",
    "#     name=\"RMSprop\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11700 samples, validate on 1300 samples\n",
      "Epoch 1/25\n",
      "11700/11700 [==============================] - 457s 39ms/sample - loss: 0.5738 - acc: 0.7323 - val_loss: 0.5760 - val_acc: 0.7273\n",
      "Epoch 2/25\n",
      "11700/11700 [==============================] - 428s 37ms/sample - loss: 0.5839 - acc: 0.7305 - val_loss: 0.5925 - val_acc: 0.7123\n",
      "Epoch 3/25\n",
      "11700/11700 [==============================] - 427s 37ms/sample - loss: 0.5669 - acc: 0.7312 - val_loss: 0.5922 - val_acc: 0.7127\n",
      "Epoch 4/25\n",
      "11700/11700 [==============================] - 440s 38ms/sample - loss: 0.5701 - acc: 0.7324 - val_loss: 0.5934 - val_acc: 0.7115\n",
      "Epoch 5/25\n",
      "11700/11700 [==============================] - 430s 37ms/sample - loss: 0.5627 - acc: 0.7313 - val_loss: 0.5606 - val_acc: 0.7123\n",
      "Epoch 6/25\n",
      "11700/11700 [==============================] - 430s 37ms/sample - loss: 0.5790 - acc: 0.7314 - val_loss: 0.6022 - val_acc: 0.7123\n",
      "Epoch 7/25\n",
      "11700/11700 [==============================] - 444s 38ms/sample - loss: 0.5791 - acc: 0.7314 - val_loss: 0.5985 - val_acc: 0.7123\n",
      "Epoch 8/25\n",
      "11700/11700 [==============================] - 432s 37ms/sample - loss: 0.5782 - acc: 0.7314 - val_loss: 0.5944 - val_acc: 0.7123\n",
      "Epoch 9/25\n",
      "11700/11700 [==============================] - 430s 37ms/sample - loss: 0.5734 - acc: 0.7314 - val_loss: 0.5788 - val_acc: 0.7123\n",
      "Epoch 10/25\n",
      "11700/11700 [==============================] - 434s 37ms/sample - loss: 0.5627 - acc: 0.7314 - val_loss: 0.5948 - val_acc: 0.7123\n",
      "Epoch 11/25\n",
      "11700/11700 [==============================] - 435s 37ms/sample - loss: 0.5007 - acc: 0.7506 - val_loss: 0.4154 - val_acc: 0.8338\n",
      "Epoch 12/25\n",
      "11700/11700 [==============================] - 430s 37ms/sample - loss: 0.3490 - acc: 0.8647 - val_loss: 0.3310 - val_acc: 0.8785\n",
      "Epoch 13/25\n",
      "11700/11700 [==============================] - 439s 38ms/sample - loss: 0.3114 - acc: 0.8812 - val_loss: 0.2981 - val_acc: 0.8846\n",
      "Epoch 14/25\n",
      "11700/11700 [==============================] - 435s 37ms/sample - loss: 0.2747 - acc: 0.8974 - val_loss: 0.2716 - val_acc: 0.8885\n",
      "Epoch 15/25\n",
      "11700/11700 [==============================] - 433s 37ms/sample - loss: 0.2527 - acc: 0.9032 - val_loss: 0.2674 - val_acc: 0.9008\n",
      "Epoch 16/25\n",
      "11700/11700 [==============================] - 442s 38ms/sample - loss: 0.2321 - acc: 0.9142 - val_loss: 0.2598 - val_acc: 0.9100\n",
      "Epoch 17/25\n",
      "11700/11700 [==============================] - 431s 37ms/sample - loss: 0.2235 - acc: 0.9148 - val_loss: 0.2534 - val_acc: 0.9085\n",
      "Epoch 18/25\n",
      "11700/11700 [==============================] - 436s 37ms/sample - loss: 0.2008 - acc: 0.9228 - val_loss: 0.2413 - val_acc: 0.9046\n",
      "Epoch 19/25\n",
      "11700/11700 [==============================] - 429s 37ms/sample - loss: 0.1843 - acc: 0.9302 - val_loss: 0.2328 - val_acc: 0.9138\n",
      "Epoch 20/25\n",
      "11700/11700 [==============================] - 426s 36ms/sample - loss: 0.1636 - acc: 0.9395 - val_loss: 0.1904 - val_acc: 0.9315\n",
      "Epoch 21/25\n",
      "11700/11700 [==============================] - 433s 37ms/sample - loss: 0.1921 - acc: 0.9241 - val_loss: 0.2646 - val_acc: 0.9115\n",
      "Epoch 22/25\n",
      "11700/11700 [==============================] - 431s 37ms/sample - loss: 0.1682 - acc: 0.9374 - val_loss: 0.1958 - val_acc: 0.9331\n",
      "Epoch 23/25\n",
      "11700/11700 [==============================] - 434s 37ms/sample - loss: 0.1430 - acc: 0.9463 - val_loss: 0.1969 - val_acc: 0.9354\n",
      "Epoch 24/25\n",
      "11700/11700 [==============================] - 434s 37ms/sample - loss: 0.1299 - acc: 0.9521 - val_loss: 0.1802 - val_acc: 0.9362\n",
      "Epoch 25/25\n",
      "11700/11700 [==============================] - 433s 37ms/sample - loss: 0.1229 - acc: 0.9547 - val_loss: 0.1805 - val_acc: 0.9369\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbc7c377d30>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.compile(loss='binary_crossentropy',\n",
    "        optimizer=opt,\n",
    "        metrics=['acc'])\n",
    "lstm_model.fit(data_train,labels_train,epochs = 25,batch_size = 128, validation_data = [data_val,labels_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11700 samples, validate on 1300 samples\n",
      "Epoch 1/5\n",
      "11700/11700 [==============================] - 436s 37ms/sample - loss: 0.1208 - acc: 0.9566 - val_loss: 0.1914 - val_acc: 0.9288\n",
      "Epoch 2/5\n",
      "11700/11700 [==============================] - 436s 37ms/sample - loss: 0.1076 - acc: 0.9608 - val_loss: 0.1875 - val_acc: 0.9377\n",
      "Epoch 3/5\n",
      "11700/11700 [==============================] - 432s 37ms/sample - loss: 0.1034 - acc: 0.9608 - val_loss: 0.1884 - val_acc: 0.9377\n",
      "Epoch 4/5\n",
      "11700/11700 [==============================] - 436s 37ms/sample - loss: 0.0963 - acc: 0.9661 - val_loss: 0.1859 - val_acc: 0.9354\n",
      "Epoch 5/5\n",
      "11700/11700 [==============================] - 434s 37ms/sample - loss: 0.1120 - acc: 0.9588 - val_loss: 0.2166 - val_acc: 0.9315\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbd787719b0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.fit(data_train,labels_train,epochs = 5,batch_size = 128, validation_data = [data_val,labels_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.save(\"./Checkpoints/lstm_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.save_weights(\"./Checkpoints/lstm_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:4081: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "lstm_model = tf.keras.models.load_model(\"./Checkpoints/lstm_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score : 0.9082462253193961\n",
      "Accuracy-score : 0.9001895135818067\n",
      "Precision : 0.8404083825900054\n",
      "Recall  : 0.9879974731522426\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "pred = lstm_model.predict(x_test)\n",
    "pred = [np.argmax(x) for x in pred]\n",
    "\n",
    "print(f\"F1-score : {f1_score([np.argmax(x) for x in y_test],pred,pos_label=1)}\")\n",
    "print(f\"Accuracy-score : {accuracy_score([np.argmax(x) for x in y_test],pred)}\")\n",
    "\n",
    "print(f\"Precision : {precision_score([np.argmax(x) for x in y_test],pred,pos_label = 1)}\")\n",
    "print(f\"Recall  : {recall_score([np.argmax(x) for x in y_test],pred,pos_label = 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pred = lstm_model.predict(x_test)\n",
    "pred = [np.argmax(x) for x in pred]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9669737137721861"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score([np.argmax(x) for x in y_test],pred,pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9535838332807073"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score([np.argmax(x) for x in y_test],pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9598572702943801"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "\n",
    "print(f\"Precision : {precision_score([np.argmax(x) for x in y_test],pred,pos_label = 1)}\")\n",
    "print(f\"Recall  : {recall_score([np.argmax(x) for x in y_test],pred,pos_label = 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for seq in data_train[128:250]:\n",
    "    for el in seq:\n",
    "        if np.isnan(el):\n",
    "            print(i)\n",
    "                \n",
    "#         if type(arr) is np.ndarray:\n",
    "#             print(i)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_pickle_file(file,file_path,protocol=None):\n",
    "    with open(file_path,\"wb\") as f:\n",
    "        if protocol:\n",
    "            pickle.dump(file,f,protocol = protocol)\n",
    "        else:\n",
    "            pickle.dump(file,f)\n",
    "\n",
    "def load_pickle_file(file_path, protocol=None):\n",
    "     with open(file_path,\"rb\") as f:\n",
    "        if protocol:\n",
    "            return pickle.load(f)\n",
    "        else:\n",
    "            return pickle.load(f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
