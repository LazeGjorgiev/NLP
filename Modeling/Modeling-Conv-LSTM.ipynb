{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_pickle_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a2e0b57b2917>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membeding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pickle_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../new_disk/embeding_matrix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mword2index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pickle_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../new_disk/word2index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mindex_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pickle_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../new_disk/index_sequences.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pickle_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./labels\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mSEQUENCE_LENGTH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m434\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_pickle_file' is not defined"
     ]
    }
   ],
   "source": [
    "embeding_matrix = load_pickle_file(\"../../new_disk/embeding_matrix\")\n",
    "word2index = load_pickle_file(\"../../new_disk/word2index\")\n",
    "index_sequences = load_pickle_file(\"../../new_disk/index_sequences.pkl\")\n",
    "labels = load_pickle_file(\"./labels\")\n",
    "SEQUENCE_LENGTH = 434\n",
    "VOCAB_SIZE = len(embeding_matrix)\n",
    "EMB_DIMENSION = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "PAD_SEQ_VALUE = 3001066"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "same_lenght_seq = [seq[:SEQUENCE_LENGTH].reshape(1,SEQUENCE_LENGTH) if len(seq) >= SEQUENCE_LENGTH \n",
    "                   else np.concatenate((seq,np.array([PAD_SEQ_VALUE] * (SEQUENCE_LENGTH - len(seq)))),axis = 0).reshape(1,SEQUENCE_LENGTH) \n",
    "                   for seq in index_sequences]\n",
    "same_lenght_seq = np.concatenate(same_lenght_seq,axis=0)\n",
    "labels = np.vstack(np.array([1,0]).reshape(1,2) if x == 0 else np.array([0,1]).reshape(1,2) for x in labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(same_lenght_seq, labels, test_size=0.2, random_state=42,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = len(x_train)\n",
    "perm = np.random.permutation(data_size)\n",
    "idx_train = perm[:int(data_size*(1-VALIDATION_SPLIT))]\n",
    "idx_val = perm[int(data_size*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "data_train = x_train[idx_train]\n",
    "labels_train = y_train[idx_train]\n",
    "\n",
    "data_val = x_train[idx_val]\n",
    "labels_val = y_train[idx_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7380, 434)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>LSTM</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras as tk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, TimeDistributed, Dropout, Activation, Flatten, Conv1D, MaxPool1D, SpatialDropout1D, GlobalMaxPool1D, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers.normalization import BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = ModelCheckpoint(\"./Checkpoints\", save_best_only=True, save_weights_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:4081: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "NUM_FILTERS = 14\n",
    "KERNEL_SIZE = 12\n",
    "HIDDEN_UNITS = 4\n",
    "rate_drop_lstm = 0.15\n",
    "rate_drop_lstm = 0.15\n",
    "LSTM_HIDDEN_DIM_SIZE = 5\n",
    "model = tk.Sequential()\n",
    "embedding_layer = Embedding(VOCAB_SIZE,\n",
    "        EMB_DIMENSION,\n",
    "        weights=[embeding_matrix],\n",
    "        input_length=SEQUENCE_LENGTH,\n",
    "        trainable=False)\n",
    "model.add(embedding_layer)\n",
    "# conv_model.add(SpatialDropout1D())\n",
    "model.add(Conv1D(NUM_FILTERS, KERNEL_SIZE, activation = 'relu',kernel_regularizer =tf.keras.regularizers.l2(l=0.09)))\n",
    "model.add(Dropout(0.2))\n",
    "# conv_model.add(BatchNormalization())\n",
    "model.add(MaxPool1D(pool_size = 12))\n",
    "\n",
    "model.add(LSTM(LSTM_HIDDEN_DIM_SIZE, activation = 'relu', dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm))\n",
    "\n",
    "# conv_model.add(MaxPool1D(pool_size = 15))\n",
    "\n",
    "# conv_model.add(GlobalMaxPool1D())\n",
    "# conv_model.add(Flatten())\n",
    "model.add(Dense(2,activation='sigmoid'))\n",
    "# lstm_model.add(LSTM(LSTM_HIDDEN_DIM_SIZE, activation = 'tanh', dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm,return_sequences = True))\n",
    "# lstm_model.add(LSTM(LSTM_HIDDEN_DIM_SIZE//2, activation = 'tanh', dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm))\n",
    "# lstm_model.add(Dense(LSTM_HIDDEN_DIM_SIZE//4, activation = 'relu'))\n",
    "# lstm_model.add(LSTM(LSTM_HIDDEN_DIM_SIZE//4, activation = 'tanh',dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del conv_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 434, 300)          900132300 \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 423, 14)           50414     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 423, 14)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 35, 14)            0         \n",
      "_________________________________________________________________\n",
      "unified_lstm (UnifiedLSTM)   (None, 5)                 400       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 12        \n",
      "=================================================================\n",
      "Total params: 900,183,126\n",
      "Trainable params: 50,826\n",
      "Non-trainable params: 900,132,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt = tk.optimizers.Adam(learning_rate=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7380 samples, validate on 821 samples\n",
      "Epoch 1/25\n",
      "7380/7380 [==============================] - 19s 3ms/sample - loss: 1.5996 - acc: 0.5106 - val_loss: 0.8030 - val_acc: 0.5670\n",
      "Epoch 2/25\n",
      "7380/7380 [==============================] - 16s 2ms/sample - loss: 0.7188 - acc: 0.5510 - val_loss: 0.6934 - val_acc: 0.5597\n",
      "Epoch 3/25\n",
      "7380/7380 [==============================] - 15s 2ms/sample - loss: 0.6910 - acc: 0.5543 - val_loss: 0.6881 - val_acc: 0.5597\n",
      "Epoch 4/25\n",
      "7380/7380 [==============================] - 16s 2ms/sample - loss: 0.6867 - acc: 0.5632 - val_loss: 0.6835 - val_acc: 0.5633\n",
      "Epoch 5/25\n",
      "7380/7380 [==============================] - 16s 2ms/sample - loss: 0.6831 - acc: 0.5716 - val_loss: 0.6831 - val_acc: 0.5621\n",
      "Epoch 6/25\n",
      "7380/7380 [==============================] - 15s 2ms/sample - loss: 0.6807 - acc: 0.5784 - val_loss: 0.6842 - val_acc: 0.5688\n",
      "Epoch 7/25\n",
      "7380/7380 [==============================] - 15s 2ms/sample - loss: 0.6805 - acc: 0.5853 - val_loss: 0.6845 - val_acc: 0.5883\n",
      "Epoch 8/25\n",
      "7380/7380 [==============================] - 16s 2ms/sample - loss: 0.7179 - acc: 0.5828 - val_loss: 0.7738 - val_acc: 0.5859\n",
      "Epoch 9/25\n",
      "7380/7380 [==============================] - 16s 2ms/sample - loss: 0.7197 - acc: 0.5909 - val_loss: 0.6679 - val_acc: 0.7777\n",
      "Epoch 10/25\n",
      "7380/7380 [==============================] - 15s 2ms/sample - loss: 0.6738 - acc: 0.6402 - val_loss: 0.6698 - val_acc: 0.7369\n",
      "Epoch 11/25\n",
      "7380/7380 [==============================] - 15s 2ms/sample - loss: 0.7210 - acc: 0.6266 - val_loss: 0.6334 - val_acc: 0.8721\n",
      "Epoch 12/25\n",
      "7380/7380 [==============================] - 15s 2ms/sample - loss: 0.6139 - acc: 0.7139 - val_loss: 0.6690 - val_acc: 0.7296\n",
      "Epoch 13/25\n",
      "7380/7380 [==============================] - 16s 2ms/sample - loss: 0.5849 - acc: 0.7524 - val_loss: 0.5765 - val_acc: 0.9208\n",
      "Epoch 14/25\n",
      "7380/7380 [==============================] - 16s 2ms/sample - loss: 0.6012 - acc: 0.7588 - val_loss: 0.6461 - val_acc: 0.9659\n",
      "Epoch 15/25\n",
      "7380/7380 [==============================] - 15s 2ms/sample - loss: 0.7377 - acc: 0.7892 - val_loss: 0.7780 - val_acc: 0.9452\n",
      "Epoch 16/25\n",
      "7380/7380 [==============================] - 16s 2ms/sample - loss: 0.6785 - acc: 0.7976 - val_loss: 0.6316 - val_acc: 0.9823\n",
      "Epoch 17/25\n",
      "7380/7380 [==============================] - 14s 2ms/sample - loss: 0.5317 - acc: 0.8309 - val_loss: 0.5698 - val_acc: 0.9903\n",
      "Epoch 18/25\n",
      "7380/7380 [==============================] - 13s 2ms/sample - loss: 0.4884 - acc: 0.8384 - val_loss: 0.5827 - val_acc: 0.9775\n",
      "Epoch 19/25\n",
      "7380/7380 [==============================] - 12s 2ms/sample - loss: 0.4610 - acc: 0.8467 - val_loss: 0.5274 - val_acc: 0.9872\n",
      "Epoch 20/25\n",
      "7380/7380 [==============================] - 14s 2ms/sample - loss: 0.4563 - acc: 0.8670 - val_loss: 0.5667 - val_acc: 0.9683\n",
      "Epoch 21/25\n",
      "7380/7380 [==============================] - 13s 2ms/sample - loss: 0.5823 - acc: 0.8267 - val_loss: 0.6272 - val_acc: 0.9708\n",
      "Epoch 22/25\n",
      "7380/7380 [==============================] - 11s 2ms/sample - loss: 0.4796 - acc: 0.8722 - val_loss: 0.5208 - val_acc: 0.9860\n",
      "Epoch 23/25\n",
      "7380/7380 [==============================] - 11s 2ms/sample - loss: 0.4277 - acc: 0.8839 - val_loss: 0.5431 - val_acc: 0.9921\n",
      "Epoch 24/25\n",
      "7380/7380 [==============================] - 11s 2ms/sample - loss: 0.4216 - acc: 0.8885 - val_loss: 0.4552 - val_acc: 0.9927\n",
      "Epoch 25/25\n",
      "7380/7380 [==============================] - 11s 2ms/sample - loss: 0.4515 - acc: 0.8860 - val_loss: 0.5536 - val_acc: 0.9963\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['acc'])\n",
    "history = model.fit(data_train,labels_train,epochs = 25,batch_size = 256, validation_data = [data_val,labels_val],shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7380 samples, validate on 821 samples\n",
      "Epoch 1/10\n",
      "7380/7380 [==============================] - 9s 1ms/sample - loss: 0.3685 - acc: 0.8527 - val_loss: 0.5241 - val_acc: 0.9963\n",
      "Epoch 2/10\n",
      "7380/7380 [==============================] - 10s 1ms/sample - loss: 0.3745 - acc: 0.8496 - val_loss: 0.5676 - val_acc: 0.9799\n",
      "Epoch 3/10\n",
      "7380/7380 [==============================] - 9s 1ms/sample - loss: 0.4469 - acc: 0.8367 - val_loss: 0.6472 - val_acc: 0.9933\n",
      "Epoch 4/10\n",
      "7380/7380 [==============================] - 9s 1ms/sample - loss: 0.4571 - acc: 0.8520 - val_loss: 0.5933 - val_acc: 0.9909\n",
      "Epoch 5/10\n",
      "7380/7380 [==============================] - 9s 1ms/sample - loss: 0.4452 - acc: 0.8474 - val_loss: 0.7497 - val_acc: 0.9543\n",
      "Epoch 6/10\n",
      "7380/7380 [==============================] - 9s 1ms/sample - loss: 0.6233 - acc: 0.8407 - val_loss: 0.7647 - val_acc: 0.9945\n",
      "Epoch 7/10\n",
      "7380/7380 [==============================] - 9s 1ms/sample - loss: 0.7763 - acc: 0.8370 - val_loss: 1.0445 - val_acc: 0.9671\n",
      "Epoch 8/10\n",
      "7380/7380 [==============================] - 9s 1ms/sample - loss: 0.7661 - acc: 0.8444 - val_loss: 0.7961 - val_acc: 0.9903\n",
      "Epoch 9/10\n",
      "7380/7380 [==============================] - 9s 1ms/sample - loss: 0.5400 - acc: 0.8537 - val_loss: 0.6123 - val_acc: 0.9957\n",
      "Epoch 10/10\n",
      "7380/7380 [==============================] - 9s 1ms/sample - loss: 0.4126 - acc: 0.8584 - val_loss: 0.5952 - val_acc: 0.9963\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(data_train,labels_train,epochs = 10,batch_size = 256, validation_data = [data_val,labels_val],shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./Checkpoints/conv_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"./Checkpoints/conv_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score : 0.9961575408261287\n",
      "Accuracy-score : 0.9960994636762555\n",
      "Precision : 0.9952015355086372\n",
      "Recall  : 0.9971153846153846\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "pred = model.predict(x_test)\n",
    "pred = [np.argmax(x) for x in pred]\n",
    "\n",
    "print(f\"F1-score : {f1_score([np.argmax(x) for x in y_test],pred,pos_label=1)}\")\n",
    "print(f\"Accuracy-score : {accuracy_score([np.argmax(x) for x in y_test],pred)}\")\n",
    "\n",
    "print(f\"Precision : {precision_score([np.argmax(x) for x in y_test],pred,pos_label = 1)}\")\n",
    "print(f\"Recall  : {recall_score([np.argmax(x) for x in y_test],pred,pos_label = 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pred = conv_model.predict(x_test)\n",
    "pred = [np.argmax(x) for x in pred]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9956751561749159"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score([np.argmax(x) for x in y_test],pred,pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9956118966357874"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score([np.argmax(x) for x in y_test],pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9956041238682188"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score([np.argmax(x) for x in y_test],pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for seq in data_train[128:250]:\n",
    "    for el in seq:\n",
    "        if np.isnan(el):\n",
    "            print(i)\n",
    "                \n",
    "#         if type(arr) is np.ndarray:\n",
    "#             print(i)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_pickle_file(file,file_path,protocol=None):\n",
    "    with open(file_path,\"wb\") as f:\n",
    "        if protocol:\n",
    "            pickle.dump(file,f,protocol = protocol)\n",
    "        else:\n",
    "            pickle.dump(file,f)\n",
    "\n",
    "def load_pickle_file(file_path, protocol=None):\n",
    "     with open(file_path,\"rb\") as f:\n",
    "        if protocol:\n",
    "            return pickle.load(f)\n",
    "        else:\n",
    "            return pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
