{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_pickle_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ced304f1abd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword2index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pickle_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../../new_disk/word2index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0membeding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pickle_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../../new_disk/embeding_matrix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mindex_sequences_titles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pickle_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./index_sequences_titles.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mindex_sequences_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pickle_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../../new_disk/index_sequences.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pickle_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../labels\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_pickle_file' is not defined"
     ]
    }
   ],
   "source": [
    "word2index = load_pickle_file(\"../../../new_disk/word2index\")\n",
    "embeding_matrix = load_pickle_file(\"../../../new_disk/embeding_matrix\")\n",
    "index_sequences_titles = load_pickle_file(\"./index_sequences_titles.pkl\")\n",
    "index_sequences_text = load_pickle_file(\"../../../new_disk/index_sequences.pkl\")\n",
    "labels = load_pickle_file(\"../labels\")\n",
    "SEQUENCE_LENGTH = 434\n",
    "SEQUENCE_LENGTH_TITLE = 11\n",
    "VOCAB_SIZE = len(embeding_matrix)\n",
    "EMB_DIMENSION = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "PAD_SEQ_VALUE = 3001066"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "same_lenght_seq = [seq[:SEQUENCE_LENGTH].reshape(1,SEQUENCE_LENGTH) if len(seq) >= SEQUENCE_LENGTH \n",
    "                   else np.concatenate((seq,np.array([PAD_SEQ_VALUE] * (SEQUENCE_LENGTH - len(seq)))),axis = 0).reshape(1,SEQUENCE_LENGTH) \n",
    "                   for seq in index_sequences_text]\n",
    "same_lenght_seq = np.concatenate(same_lenght_seq,axis=0)\n",
    "\n",
    "same_lenght_titles = [seq[:SEQUENCE_LENGTH_TITLE].reshape(1,SEQUENCE_LENGTH_TITLE) if len(seq) >= SEQUENCE_LENGTH_TITLE \n",
    "                   else np.concatenate((seq,np.array([PAD_SEQ_VALUE] * (SEQUENCE_LENGTH_TITLE - len(seq)))),axis = 0).reshape(1,SEQUENCE_LENGTH_TITLE) \n",
    "                   for seq in index_sequences_titles]\n",
    "same_lenght_titles = np.concatenate(same_lenght_titles,axis=0)\n",
    "same_lenght_titles = np.array([[int(x) for x in l] for l in same_lenght_titles])\n",
    "labels = np.vstack(np.array([1,0]).reshape(1,2) if x == 0 else np.array([0,1]).reshape(1,2) for x in labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_indexes, x_test_indexes, y_train, y_test = train_test_split(list(range(len(same_lenght_seq))), \n",
    "                                                                                    labels, test_size=0.2,\n",
    "                                                                                    random_state=42,\n",
    "                                                                                    shuffle = True)\n",
    "x_train_text = same_lenght_seq[x_train_indexes]\n",
    "x_train_title = same_lenght_titles[x_train_indexes]\n",
    "\n",
    "x_test_text = same_lenght_seq[x_test_indexes]\n",
    "x_test_title = same_lenght_titles[x_test_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create validation dataset\n",
    "data_size = len(x_train_text)\n",
    "perm = np.random.permutation(data_size)\n",
    "idx_train = perm[:int(data_size*(1-VALIDATION_SPLIT))]\n",
    "idx_val = perm[int(data_size*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "data_train_text = x_train_text[idx_train]\n",
    "data_train_title = x_train_title[idx_train]\n",
    "\n",
    "labels_train = y_train[idx_train]\n",
    "\n",
    "data_val_text = x_train_text[idx_val]\n",
    "data_val_title = x_train_title[idx_val]\n",
    "\n",
    "labels_val = y_train[idx_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7380, 434)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Convolutional neural networks</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras as tk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Flatten, Conv1D, MaxPool1D, SpatialDropout1D, GlobalMaxPool1D, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers.normalization import BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = ModelCheckpoint(\"./Checkpoints\", save_best_only=True, save_weights_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11397, 618)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:4081: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "#Text model\n",
    "LSTM_HIDDEN_DIM_SIZE = 64\n",
    "rate_drop_lstm = 0.15\n",
    "rate_drop_lstm = 0.15\n",
    "input_1 = Input(shape=(SEQUENCE_LENGTH,))\n",
    "embedding_layer = Embedding(VOCAB_SIZE,\n",
    "        EMB_DIMENSION,\n",
    "        weights=[embeding_matrix],\n",
    "        input_length=SEQUENCE_LENGTH,\n",
    "        trainable=False)(input_1)\n",
    "\n",
    "x1 = LSTM(LSTM_HIDDEN_DIM_SIZE, activation = 'tanh', dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm,return_sequences = True)(embedding_layer)\n",
    "x1 = LSTM(LSTM_HIDDEN_DIM_SIZE//2, activation = 'tanh', dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)(x1)\n",
    "x1 = Dense(LSTM_HIDDEN_DIM_SIZE//4, activation = 'tanh')(x1)\n",
    "\n",
    "#Titles model\n",
    "LSTM_HIDDEN_DIM_SIZE = 4\n",
    "rate_drop_lstm = 0.15\n",
    "rate_drop_lstm = 0.15\n",
    "\n",
    "input_2 = Input(shape=(SEQUENCE_LENGTH_TITLE,))\n",
    "embedding_layer = Embedding(VOCAB_SIZE,\n",
    "        EMB_DIMENSION,\n",
    "        weights=[embeding_matrix],\n",
    "        input_length=SEQUENCE_LENGTH_TITLE,\n",
    "        trainable=False)(input_2)\n",
    "\n",
    "\n",
    "x2 = LSTM(LSTM_HIDDEN_DIM_SIZE, activation = 'tanh', dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm,return_sequences = True)(embedding_layer)\n",
    "x2 = Dropout(0.3)(x2)\n",
    "x2 = LSTM(LSTM_HIDDEN_DIM_SIZE//2, activation = 'tanh', dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)(x2)\n",
    "\n",
    "concat = tk.layers.Concatenate()([x1,x2])\n",
    "pre_output = Dense(5,activation='relu')(concat)\n",
    "output = Dense(2,activation='sigmoid')(pre_output)\n",
    "full_model = tf.keras.Model(inputs=[input_1, input_2], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 434)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 11)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 434, 300)     900132300   input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 11, 300)      900132300   input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "unified_lstm (UnifiedLSTM)      (None, 434, 64)      93440       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "unified_lstm_2 (UnifiedLSTM)    (None, 11, 4)        4880        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "unified_lstm_1 (UnifiedLSTM)    (None, 32)           12416       unified_lstm[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 11, 4)        0           unified_lstm_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           528         unified_lstm_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "unified_lstm_3 (UnifiedLSTM)    (None, 2)            56          dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 18)           0           dense[0][0]                      \n",
      "                                                                 unified_lstm_3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 5)            95          concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            12          dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,800,376,027\n",
      "Trainable params: 111,427\n",
      "Non-trainable params: 1,800,264,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "full_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "del full_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tk.optimizers.Adam(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7380 samples, validate on 821 samples\n",
      "Epoch 1/20\n",
      "7380/7380 [==============================] - 100s 14ms/sample - loss: 0.6873 - acc: 0.5682 - val_loss: 0.6594 - val_acc: 0.8167\n",
      "Epoch 2/20\n",
      "7380/7380 [==============================] - 93s 13ms/sample - loss: 0.5421 - acc: 0.8329 - val_loss: 0.3469 - val_acc: 0.8770\n",
      "Epoch 3/20\n",
      "7380/7380 [==============================] - 86s 12ms/sample - loss: 0.3003 - acc: 0.8929 - val_loss: 0.2975 - val_acc: 0.8916\n",
      "Epoch 4/20\n",
      "7380/7380 [==============================] - 88s 12ms/sample - loss: 0.2593 - acc: 0.9053 - val_loss: 0.2798 - val_acc: 0.9026\n",
      "Epoch 5/20\n",
      "7380/7380 [==============================] - 94s 13ms/sample - loss: 0.2366 - acc: 0.9173 - val_loss: 0.2789 - val_acc: 0.9038\n",
      "Epoch 6/20\n",
      "7380/7380 [==============================] - 87s 12ms/sample - loss: 0.2283 - acc: 0.9172 - val_loss: 0.2634 - val_acc: 0.9050\n",
      "Epoch 7/20\n",
      "7380/7380 [==============================] - 88s 12ms/sample - loss: 0.2067 - acc: 0.9264 - val_loss: 0.2492 - val_acc: 0.9093\n",
      "Epoch 8/20\n",
      "7380/7380 [==============================] - 88s 12ms/sample - loss: 0.2036 - acc: 0.9247 - val_loss: 0.2411 - val_acc: 0.9099\n",
      "Epoch 9/20\n",
      "7380/7380 [==============================] - 90s 12ms/sample - loss: 0.1981 - acc: 0.9280 - val_loss: 0.2490 - val_acc: 0.9111\n",
      "Epoch 10/20\n",
      "7380/7380 [==============================] - 87s 12ms/sample - loss: 0.1954 - acc: 0.9308 - val_loss: 0.2569 - val_acc: 0.9050\n",
      "Epoch 11/20\n",
      "7380/7380 [==============================] - 86s 12ms/sample - loss: 0.1934 - acc: 0.9285 - val_loss: 0.2312 - val_acc: 0.9135\n",
      "Epoch 12/20\n",
      "7380/7380 [==============================] - 88s 12ms/sample - loss: 0.1811 - acc: 0.9341 - val_loss: 0.2299 - val_acc: 0.9123\n",
      "Epoch 13/20\n",
      "7380/7380 [==============================] - 88s 12ms/sample - loss: 0.1803 - acc: 0.9335 - val_loss: 0.2315 - val_acc: 0.9111\n",
      "Epoch 14/20\n",
      "7380/7380 [==============================] - 86s 12ms/sample - loss: 0.1766 - acc: 0.9356 - val_loss: 0.2370 - val_acc: 0.9153\n",
      "Epoch 15/20\n",
      "7380/7380 [==============================] - 90s 12ms/sample - loss: 0.1746 - acc: 0.9372 - val_loss: 0.2260 - val_acc: 0.9166\n",
      "Epoch 16/20\n",
      "7380/7380 [==============================] - 91s 12ms/sample - loss: 0.1656 - acc: 0.9392 - val_loss: 0.2337 - val_acc: 0.9184\n",
      "Epoch 17/20\n",
      "7380/7380 [==============================] - 90s 12ms/sample - loss: 0.1636 - acc: 0.9401 - val_loss: 0.2373 - val_acc: 0.9129\n",
      "Epoch 18/20\n",
      "7380/7380 [==============================] - 85s 12ms/sample - loss: 0.1687 - acc: 0.9384 - val_loss: 0.2235 - val_acc: 0.9239\n",
      "Epoch 19/20\n",
      "7380/7380 [==============================] - 87s 12ms/sample - loss: 0.1561 - acc: 0.9442 - val_loss: 0.2402 - val_acc: 0.9196\n",
      "Epoch 20/20\n",
      "7380/7380 [==============================] - 91s 12ms/sample - loss: 0.1632 - acc: 0.9409 - val_loss: 0.2300 - val_acc: 0.9220\n"
     ]
    }
   ],
   "source": [
    "full_model.compile(loss='binary_crossentropy',\n",
    "        optimizer=opt,\n",
    "        metrics=['acc'])\n",
    "history2 = full_model.fit([data_train_text,data_train_title],labels_train,epochs = 20,batch_size = 256, \n",
    "                     validation_data = [[data_val_text,data_val_title],labels_val],shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7380 samples, validate on 821 samples\n",
      "Epoch 1/5\n",
      "7380/7380 [==============================] - 89s 12ms/sample - loss: 0.1190 - acc: 0.9591 - val_loss: 0.1867 - val_acc: 0.9245\n",
      "Epoch 2/5\n",
      "7380/7380 [==============================] - 86s 12ms/sample - loss: 0.1221 - acc: 0.9568 - val_loss: 0.2094 - val_acc: 0.9312\n",
      "Epoch 3/5\n",
      "7380/7380 [==============================] - 85s 12ms/sample - loss: 0.1128 - acc: 0.9601 - val_loss: 0.2028 - val_acc: 0.9306\n",
      "Epoch 4/5\n",
      "7380/7380 [==============================] - 85s 11ms/sample - loss: 0.1135 - acc: 0.9609 - val_loss: 0.2066 - val_acc: 0.9294\n",
      "Epoch 5/5\n",
      "7380/7380 [==============================] - 93s 13ms/sample - loss: 0.1130 - acc: 0.9591 - val_loss: 0.2147 - val_acc: 0.9269\n"
     ]
    }
   ],
   "source": [
    "history2 = full_model.fit([data_train_text,data_train_title],labels_train,epochs = 5,batch_size = 256, \n",
    "                     validation_data = [[data_val_text,data_val_title],labels_val],shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model.save(\"./Checkpoints/conv_model2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model.save_weights(\"./Checkpoints/conv_weights2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score : 0.9263157894736842\n",
      "Accuracy-score : 0.9283276450511946\n",
      "Precision : 0.9675392670157068\n",
      "Recall  : 0.8884615384615384\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "pred = full_model.predict([x_test_text,x_test_title])\n",
    "pred = [np.argmax(x) for x in pred]\n",
    "\n",
    "print(f\"F1-score : {f1_score([np.argmax(x) for x in y_test],pred,pos_label=1)}\")\n",
    "print(f\"Accuracy-score : {accuracy_score([np.argmax(x) for x in y_test],pred)}\")\n",
    "\n",
    "print(f\"Precision : {precision_score([np.argmax(x) for x in y_test],pred,pos_label = 1)}\")\n",
    "print(f\"Recall  : {recall_score([np.argmax(x) for x in y_test],pred,pos_label = 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9362930077691454"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score([np.argmax(x) for x in y_test],pred,pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9093779602147143"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score([np.argmax(x) for x in y_test],pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8797661587489404"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score([np.argmax(x) for x in y_test],pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 0.8851894374282434\n",
      "Recall  : 0.8048016701461378\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "\n",
    "print(f\"Precision : {precision_score([np.argmax(x) for x in y_test],pred,pos_label = 0)}\")\n",
    "print(f\"Recall  : {recall_score([np.argmax(x) for x in y_test],pred,pos_label = 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_pickle_file(file,file_path,protocol=None):\n",
    "    with open(file_path,\"wb\") as f:\n",
    "        if protocol:\n",
    "            pickle.dump(file,f,protocol = protocol)\n",
    "        else:\n",
    "            pickle.dump(file,f)\n",
    "\n",
    "def load_pickle_file(file_path, protocol=None):\n",
    "     with open(file_path,\"rb\") as f:\n",
    "        if protocol:\n",
    "            return pickle.load(f)\n",
    "        else:\n",
    "            return pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
