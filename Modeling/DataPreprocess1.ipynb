{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wv21VXQNsJ25",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# np.load(\"/content/drive/My Drive/ModelingAllLogs (1)/template2vec_hdfs_logs.npy\",allow_pickle=True)\n",
    "corpus_fake = pd.read_csv(\"../dataset/Fake.csv\")\n",
    "corpus_fake = corpus_fake[[True if len(x) > 60 else False for x in corpus_fake[\"text\"].values]]\n",
    "corpus_true = pd.read_csv(\"../dataset/True.csv\")\n",
    "corpus_true = corpus_true[[True if len(x) > 60 else False for x in corpus_true[\"text\"].values]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "onaya1mFY94h"
   },
   "outputs": [],
   "source": [
    "month_to_number = {\"December\":12,\"November\":11,\"October\":10,\"September\":9,\"August\":8,\"July\":7,\"June\":6,\n",
    "                   \"May\":5,\"April\":3,\"March\":3,\"February\":2,\"January\":1,\"Feb\":2,\"Dec\":12,\"Nov\":11,\"Oct\":10,\n",
    "                   \"Sep\":9,\"Aug\":8,\"Jul\":7,\"Jun\":6,\"Apr\":4,\"Mar\":3,\"Jan\":1}\n",
    "\n",
    "dates = []\n",
    "to_delete= []\n",
    "for i in range(corpus_fake.shape[0]):\n",
    "   t = corpus_fake.date.iloc[i]\n",
    "   try:\n",
    "      if \"-\" in t:\n",
    "        p = pd.to_datetime(t.split(\"-\")[0] +\":\" + str(month_to_number[t.split(\"-\")[1]]) + \":\"+ t.split(\"-\")[-1],format=\"%d:%m:%y\")\n",
    "      else:     \n",
    "        p =pd.to_datetime(t.split(\" \")[1][:-1] +\":\" + str(month_to_number[t.split(\" \")[0]]) + t.split(\",\")[-1],format='%d:%m %Y')\n",
    "      dates.append(p)\n",
    "   except:\n",
    "     to_delete.append(i)\n",
    "for i in to_delete:\n",
    "  corpus_fake.drop(index = i,inplace=True)\n",
    "corpus_fake[\"date\"] = dates\n",
    "\n",
    "dates = []\n",
    "to_delete= []\n",
    "for i in range(corpus_true.shape[0]):\n",
    "    t = corpus_true.date.iloc[i]\n",
    "    try:\n",
    "        if \"-\" in t:\n",
    "            p = pd.to_datetime(t.split(\"-\")[0] +\":\" + str(month_to_number[t.split(\"-\")[1]]) + \":\"+ t.split(\"-\")[-1],format=\"%d:%m:%y\")\n",
    "        else:     \n",
    "            p =pd.to_datetime(t.split(\" \")[1][:-1] +\":\" + str(month_to_number[t.split(\" \")[0]]) + t.split(\",\")[-1][:-1],format='%d:%m %Y')\n",
    "        dates.append(p)\n",
    "    except:\n",
    "        print(i)\n",
    "        to_delete.append(i)\n",
    "for i in to_delete:\n",
    "    corpus_true.drop(index = i,inplace=True)\n",
    "corpus_true[\"date\"] = dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bRf_7zMxhH76"
   },
   "outputs": [],
   "source": [
    "fake_news_us_el = corpus_fake[(corpus_fake[\"date\"] > pd.to_datetime('01:08:2016', format='%d:%m:%Y')) & \n",
    "            (corpus_fake[\"date\"] < pd.to_datetime('12:12:2016', format='%d:%m:%Y')) &\n",
    "            (corpus_fake[\"subject\"] == \"politics\")]\n",
    "\n",
    "true_news_us_el = corpus_true[(corpus_true[\"date\"] > pd.to_datetime('22:09:2016', format='%d:%m:%Y')) & \n",
    "            (corpus_true[\"date\"] < pd.to_datetime('27:11:2016', format='%d:%m:%Y')) &\n",
    "            (corpus_true[\"subject\"] == \"politicsNews\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "VjIMs_ORi9WD",
    "outputId": "f32d7e17-0ae7-4564-8c45-9b87afe20328"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1005, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1002, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(true_news_us_el.shape)\n",
    "fake_news_us_el.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "jfqlPgUFxAJt",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "ed0c7506-f4a7-4e61-8fc0-efd33204e001"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laze/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/laze/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "fake_news_us_el[\"label\"] = [1] * fake_news_us_el.shape[0]\n",
    "true_news_us_el[\"label\"] = [0] * true_news_us_el.shape[0]\n",
    "# all_data = fake_news_us_el.append(true_news_us_el)\n",
    "# all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "K3HHfrV23fyr",
    "outputId": "295aeee3-3572-45f7-ce66-239d3bd70fd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total true news 1002\n",
      "Total fake news 1005\n"
     ]
    }
   ],
   "source": [
    "print(\"Total true news \"+str(fake_news_us_el.shape[0]))\n",
    "print(\"Total fake news \"+str(true_news_us_el.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kBtDgVe33pNR"
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "lemmetizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "text_prepared_fake = []\n",
    "for t in fake_news_us_el.text.values.tolist():\n",
    "    v= []\n",
    "    for w in tokenizer.tokenize(t):\n",
    "        w_lem = lemmetizer.lemmatize(w.lower())\n",
    "        if not w_lem.isdigit() and w_lem not in stopwords.words('english') and not bool(re.search(r'\\d', w_lem)):\n",
    "            v.append(w_lem)\n",
    "    text_prepared_fake.append(v)\n",
    "    \n",
    "text_prepared_true = []\n",
    "for t in true_news_us_el.text.values.tolist():\n",
    "    v= []\n",
    "    for w in tokenizer.tokenize(t):\n",
    "        w_lem = lemmetizer.lemmatize(w.lower())\n",
    "        if not w_lem.isdigit() and w_lem not in stopwords.words('english') and not bool(re.search(r'\\d', w_lem)):\n",
    "            v.append(w_lem)\n",
    "    text_prepared_true.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named nltk",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1f4ae00a6652>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mngrams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbigrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_prepared_fake\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mall_bigrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbigrams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named nltk"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "bigrams = [list(ngrams(t,2)) for t in text_prepared_fake[:]]\n",
    "all_bigrams = set()\n",
    "\n",
    "for t in bigrams:\n",
    "    for b_gram in t:\n",
    "        all_bigrams.add(\" \".join(b_gram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1000_bigrams = list(get_most_frequent_terms(bigrams,5000).keys())\n",
    "top_2000_unigrams = list(get_most_frequent_terms(text_prepared[2:],2000).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1002"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-rc1'"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors,not_found = compute_td_idf_fatures(top_1000_bigrams,bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>LSTM</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "<h1>VAE</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_and_train_vae_model(np.asarray(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "\n",
    "def create_and_train_vae_model(train_data,epochs = 5):\n",
    "    original_dimension=train_data.shape[1]\n",
    "    # original_dimension_rec=len(include)\n",
    "    original_dimension_rec=train_data.shape[1]\n",
    "    train_data = train_data.reshape(train_data.shape[0],1,original_dimension)\n",
    "\n",
    "    hidden_unit_dimension=64\n",
    "    latent_dim=30\n",
    "    shape=(1,original_dimension)\n",
    "    batch_shape=32\n",
    "    #We assume that the prior distribution of the latent space  is isotropic gausiian distribution\n",
    "    prior=tfp.distributions.Independent(tfp.distributions.Normal(loc=tf.zeros(latent_dim),scale=1),\n",
    "                                                     reinterpreted_batch_ndims=1)\n",
    "\n",
    "    #encoder\n",
    "    encoder=tfk.Sequential()\n",
    "    encoder.add(tfkl.InputLayer(input_shape=shape))\n",
    "    encoder.add(tfkl.Dense(units=hidden_unit_dimension,activation='tanh',kernel_regularizer='l2',activity_regularizer='l2'))\n",
    "    encoder.add(tfkl.Dense(units=hidden_unit_dimension//2,activation='tanh',kernel_regularizer='l2'))\n",
    "    encoder.add(tfkl.Dense(units=tfp.layers.MultivariateNormalTriL.params_size(latent_dim),activation='tanh',kernel_regularizer='l2'))\n",
    "    encoder.add(tfp.layers.MultivariateNormalTriL(latent_dim,\n",
    "                                                  activity_regularizer=tfp.layers.KLDivergenceRegularizer(prior,\n",
    "                                                                                                              weight=1.0)))\n",
    "    # encoder=tfk.Model(input_layer,layers)\n",
    "\n",
    "    #decoder\n",
    "    decoder=tfk.Sequential()\n",
    "    decoder.add(tfkl.InputLayer(input_shape=[latent_dim]))\n",
    "    decoder.add(tfkl.Reshape([1,1,latent_dim]))\n",
    "    decoder.add(tfkl.Dense(units=hidden_unit_dimension,activation='tanh',kernel_regularizer='l2',activity_regularizer='l2'))\n",
    "    decoder.add(tfkl.Dense(units=hidden_unit_dimension//2,activation='tanh'))\n",
    "    decoder.add(tfkl.Dense(units=tfp.layers.MultivariateNormalTriL.params_size(original_dimension_rec),activation='tanh'))\n",
    "    decoder.add(tfp.layers.MultivariateNormalTriL(original_dimension_rec))\n",
    "\n",
    "\n",
    "    vae=tfk.Model(encoder.inputs,outputs=decoder(encoder.outputs[0]))\n",
    "\n",
    "    def mahalanobis_distance(x,rv_x):\n",
    "\n",
    "            curr_batch_shape=K.int_shape(x)[0]\n",
    "            covariance=rv_x.covariance()\n",
    "            sample_vector=tf.reduce_mean(rv_x.sample(5),axis=0)\n",
    "            if(curr_batch_shape is None):\n",
    "                curr_batch_shape=-1\n",
    "            elif(not check_PSD(covariance.numpy())):\n",
    "                    covariance=nearPSDTensorflow(covariance)\n",
    "\n",
    "            inv_cov_matrix = tf.reshape(tf.linalg.inv(covariance),[curr_batch_shape,original_dimension_rec,original_dimension_rec])\n",
    "            diff = tf.reshape(tf.math.subtract(tf.reshape(x,[curr_batch_shape,1,1,original_dimension_rec]),sample_vector),[curr_batch_shape,original_dimension_rec])\n",
    "            matVec=tf.linalg.matvec(inv_cov_matrix,diff)    \n",
    "            l1=tf.reduce_sum(tf.multiply(matVec,diff),axis=1)\n",
    "            return l1\n",
    "\n",
    "\n",
    "    def check_PSD(M):\n",
    "        return np.all(np.linalg.eigvals(M)>=0)\n",
    "    vae.compile(optimizer='adam',loss=mahalanobis_distance)\n",
    "    vae.fit(train_data,train_data,batch_size=32,epochs = epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146, 10000)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(vectors).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_td_idf_fatures(vocabulary :list, documents :list):\n",
    "    word_to_index = {k:v for v,k in enumerate(top_1000_bigrams)}\n",
    "    not_found = []\n",
    "    td_idf_features = []\n",
    "    i = 0\n",
    "    for doc in documents:\n",
    "        vector = np.zeros(len(vocabulary))\n",
    "        #computing idf\n",
    "        idf_per_word = {}\n",
    "        j = 0\n",
    "        for w in set(doc):\n",
    "            count = 1\n",
    "            for d in documents:\n",
    "                if w in d:\n",
    "                    count+=1\n",
    "            idf_per_word[w] = np.log(len(documents)/count)\n",
    "        #computing tf\n",
    "        for w in doc:\n",
    "            if w not in word_to_index:\n",
    "                not_found.append(w)\n",
    "                continue\n",
    "            idx = word_to_index[w]\n",
    "            vector[idx] = vector[idx] + 1\n",
    "        vector = vector/len(doc)        \n",
    "        for w in doc:\n",
    "            if w not in word_to_index:\n",
    "                continue\n",
    "            idx = word_to_index[w]\n",
    "            vector[idx] = vector[idx] * idf_per_word[w]\n",
    "        td_idf_features.append(vector)\n",
    "    return td_idf_features,not_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P2PpCUaI3DVw"
   },
   "outputs": [],
   "source": [
    "def get_most_frequent_terms(terms,top_n):\n",
    "    term_to_freq = {}\n",
    "    for t in terms:\n",
    "        for w in t:\n",
    "            if w in term_to_freq:\n",
    "                term_to_freq[w] = term_to_freq[w] + 1\n",
    "            else:\n",
    "                term_to_freq[w] = 1\n",
    "     \n",
    "    res = {k: v for k, v in sorted(term_to_freq.items(), key=lambda item: item[1],reverse=True)}\n",
    "    return {k:term_to_freq[k] for k in list(res)[:top_n]}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
