{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeding_matrix = load_pickle_file(\"../new_disk/embeding_matrix\")\n",
    "word2index = load_pickle_file(\"../new_disk/word2index\")\n",
    "index_sequences = load_pickle_file(\"../new_disk/index_sequences.pkl\")\n",
    "labels = load_pickle_file(\"../Modeling/labels\")\n",
    "SEQUENCE_LENGTH = 434\n",
    "VOCAB_SIZE = len(embeding_matrix)\n",
    "EMB_DIMENSION = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "PAD_SEQ_VALUE = 3000440"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "same_lenght_seq = [seq[:SEQUENCE_LENGTH].reshape(1,SEQUENCE_LENGTH) if len(seq) >= SEQUENCE_LENGTH \n",
    "                   else np.concatenate((seq,np.array([PAD_SEQ_VALUE] * (SEQUENCE_LENGTH - len(seq)))),axis = 0).reshape(1,SEQUENCE_LENGTH) \n",
    "                   for seq in index_sequences]\n",
    "same_lenght_seq = np.concatenate(same_lenght_seq,axis=0)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(same_lenght_seq, labels, test_size=0.2, random_state=42,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = len(x_train)\n",
    "perm = np.random.permutation(data_size)\n",
    "idx_train = perm[:int(data_size*(1-VALIDATION_SPLIT))]\n",
    "idx_val = perm[int(data_size*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "data_train = x_train[idx_train]\n",
    "labels_train = y_train[idx_train]\n",
    "\n",
    "data_val = x_train[idx_val]\n",
    "labels_val = y_train[idx_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7380, 434)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>LSTM</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras as tk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers.normalization import BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = ModelCheckpoint(\"./\", save_best_only=True, save_weights_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_HIDDEN_DIM_SIZE = 126\n",
    "rate_drop_lstm = 0.15\n",
    "rate_drop_lstm = 0.15\n",
    "lstm_model = tk.Sequential()\n",
    "embedding_layer = Embedding(VOCAB_SIZE,\n",
    "        EMB_DIMENSION,\n",
    "        weights=[embeding_matrix],\n",
    "        input_length=SEQUENCE_LENGTH,\n",
    "        trainable=False)\n",
    "lstm_model.add(embedding_layer)\n",
    "lstm_model.add(LSTM(LSTM_HIDDEN_DIM_SIZE, activation = 'tanh', dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm,return_sequences = True))\n",
    "lstm_model.add(LSTM(LSTM_HIDDEN_DIM_SIZE//2, activation = 'tanh', dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm,return_sequences = True))\n",
    "lstm_model.add(LSTM(LSTM_HIDDEN_DIM_SIZE//2, activation = 'tanh',dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm))\n",
    "lstm_model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tk.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7380 samples, validate on 821 samples\n",
      "Epoch 1/10\n",
      "7360/7380 [============================>.] - ETA: 2s - loss: 0.6815 - acc: 0.5484"
     ]
    }
   ],
   "source": [
    "lstm_model.compile(loss='binary_crossentropy',\n",
    "        optimizer=opt,\n",
    "        metrics=['acc'])\n",
    "lstm_model.fit(data_train,labels_train,epochs = 10,batch_size = 32, validation_data = [data_val,labels_val], callbacks = [model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   3563,       1,    5673,    1283,     185,     186,     635,\n",
       "            26,      17,     257,     373,     343,     338,    2500,\n",
       "             7,       1,    9552,       2,      10,     195,    1370,\n",
       "          1939,      43,     983,      61,     489,   11653,       7,\n",
       "             1,     353,     257,     373,   18571,    7751,   18298,\n",
       "          1816,      28,     120,     523,     167,       7,       1,\n",
       "           881,     287,      71,    3196,    7751,       5,     541,\n",
       "           411,     168,     667,       7,    2741,   18298,      26,\n",
       "            17,       4,    9411,     221,      19,       7,       1,\n",
       "            80,    2811,     242,    2133,     134,    2133,     312,\n",
       "           122,     250,     164,    1702,     349,     758,     279,\n",
       "          1132,     316,     186,     635,     281,       5,    1085,\n",
       "          2261,     166,     403,     449,      14,    1138,      54,\n",
       "           182,    1404,     132,      58,     357, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440,\n",
       "       3000440, 3000440, 3000440, 3000440, 3000440, 3000440, 3000440])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for seq in data_train[128:250]:\n",
    "    for el in seq:\n",
    "        if np.isnan(el):\n",
    "            print(i)\n",
    "                \n",
    "#         if type(arr) is np.ndarray:\n",
    "#             print(i)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_pickle_file(file,file_path,protocol=None):\n",
    "    with open(file_path,\"wb\") as f:\n",
    "        if protocol:\n",
    "            pickle.dump(file,f,protocol = protocol)\n",
    "        else:\n",
    "            pickle.dump(file,f)\n",
    "\n",
    "def load_pickle_file(file_path, protocol=None):\n",
    "     with open(file_path,\"rb\") as f:\n",
    "        if protocol:\n",
    "            return pickle.load(f)\n",
    "        else:\n",
    "            return pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
