{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wv21VXQNsJ25"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# np.load(\"/content/drive/My Drive/ModelingAllLogs (1)/template2vec_hdfs_logs.npy\",allow_pickle=True)\n",
    "corpus_fake = pd.read_csv(\"../dataset/Fake.csv\")\n",
    "corpus_fake = corpus_fake[[True if len(x) > 60 else False for x in corpus_fake[\"text\"].values]]\n",
    "corpus_true = pd.read_csv(\"../dataset/True.csv\")\n",
    "corpus_true = corpus_true[[True if len(x) > 60 else False for x in corpus_true[\"text\"].values]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "onaya1mFY94h"
   },
   "outputs": [],
   "source": [
    "month_to_number = {\"December\":12,\"November\":11,\"October\":10,\"September\":9,\"August\":8,\"July\":7,\"June\":6,\n",
    "                   \"May\":5,\"April\":3,\"March\":3,\"February\":2,\"January\":1,\"Feb\":2,\"Dec\":12,\"Nov\":11,\"Oct\":10,\n",
    "                   \"Sep\":9,\"Aug\":8,\"Jul\":7,\"Jun\":6,\"Apr\":4,\"Mar\":3,\"Jan\":1}\n",
    "\n",
    "dates = []\n",
    "to_delete= []\n",
    "for i in range(corpus_fake.shape[0]):\n",
    "   t = corpus_fake.date.iloc[i]\n",
    "   try:\n",
    "      if \"-\" in t:\n",
    "        p = pd.to_datetime(t.split(\"-\")[0] +\":\" + str(month_to_number[t.split(\"-\")[1]]) + \":\"+ t.split(\"-\")[-1],format=\"%d:%m:%y\")\n",
    "      else:     \n",
    "        p =pd.to_datetime(t.split(\" \")[1][:-1] +\":\" + str(month_to_number[t.split(\" \")[0]]) + t.split(\",\")[-1],format='%d:%m %Y')\n",
    "      dates.append(p)\n",
    "   except:\n",
    "     to_delete.append(i)\n",
    "for i in to_delete:\n",
    "  corpus_fake.drop(index = i,inplace=True)\n",
    "corpus_fake[\"date\"] = dates\n",
    "\n",
    "dates = []\n",
    "to_delete= []\n",
    "for i in range(corpus_true.shape[0]):\n",
    "    t = corpus_true.date.iloc[i]\n",
    "    try:\n",
    "        if \"-\" in t:\n",
    "            p = pd.to_datetime(t.split(\"-\")[0] +\":\" + str(month_to_number[t.split(\"-\")[1]]) + \":\"+ t.split(\"-\")[-1],format=\"%d:%m:%y\")\n",
    "        else:     \n",
    "            p =pd.to_datetime(t.split(\" \")[1][:-1] +\":\" + str(month_to_number[t.split(\" \")[0]]) + t.split(\",\")[-1][:-1],format='%d:%m %Y')\n",
    "        dates.append(p)\n",
    "    except:\n",
    "        print(i)\n",
    "        to_delete.append(i)\n",
    "for i in to_delete:\n",
    "    corpus_true.drop(index = i,inplace=True)\n",
    "corpus_true[\"date\"] = dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bRf_7zMxhH76"
   },
   "outputs": [],
   "source": [
    "fake_news_us_el = corpus_fake[(corpus_fake[\"date\"] > pd.to_datetime('01:08:2016', format='%d:%m:%Y')) & \n",
    "            (corpus_fake[\"date\"] < pd.to_datetime('12:12:2016', format='%d:%m:%Y')) &\n",
    "            (corpus_fake[\"subject\"] == \"politics\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lfTNmSqIYms9"
   },
   "outputs": [],
   "source": [
    "true_news_us_el = corpus_true[(corpus_true[\"date\"] > pd.to_datetime('22:09:2016', format='%d:%m:%Y')) & \n",
    "            (corpus_true[\"date\"] < pd.to_datetime('27:11:2016', format='%d:%m:%Y')) &\n",
    "            (corpus_true[\"subject\"] == \"politicsNews\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "VjIMs_ORi9WD",
    "outputId": "f32d7e17-0ae7-4564-8c45-9b87afe20328"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1005, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1002, 4)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(true_news_us_el.shape)\n",
    "fake_news_us_el.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "jfqlPgUFxAJt",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "ed0c7506-f4a7-4e61-8fc0-efd33204e001"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laze/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/laze/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "fake_news_us_el[\"label\"] = [1] * fake_news_us_el.shape[0]\n",
    "true_news_us_el[\"label\"] = [0] * true_news_us_el.shape[0]\n",
    "# all_data = fake_news_us_el.append(true_news_us_el)\n",
    "# all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s5S7Yt1026gk"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmetizer = WordNetLemmatizer()\n",
    "corputs_of_words = {}\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "all_words=set()\n",
    "word_to_index = {}\n",
    "text = all_data.text.values.tolist()\n",
    "i=0   \n",
    "for t in text:\n",
    "    words = [lemmetizer.lemmatize(w.lower()) for w in tokenizer.tokenize(t) if not w.isdigit()]\n",
    "    for w in words:\n",
    "        if w not in word_to_index.keys():\n",
    "            all_words.add(w)\n",
    "            word_to_index[w] = i\n",
    "            i+=1\n",
    "titles = all_data.title.values.tolist()\n",
    "for t in titles:\n",
    "    words = [lemmetizer.lemmatize(w.lower()) for w in tokenizer.tokenize(t) if not w.isdigit()]\n",
    "    for w in words:\n",
    "         if w not in word_to_index.keys():\n",
    "            all_words.add(w)\n",
    "            word_to_index[w] = i\n",
    "            i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "K3HHfrV23fyr",
    "outputId": "295aeee3-3572-45f7-ce66-239d3bd70fd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total true news 1002\n",
      "Total fake news 1005\n"
     ]
    }
   ],
   "source": [
    "print(\"Total true news \"+str(fake_news_us_el.shape[0]))\n",
    "print(\"Total fake news \"+str(true_news_us_el.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kBtDgVe33pNR"
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "lemmetizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "# text_prepared = []\n",
    "# for t in all_data.text.values.tolist():\n",
    "#     v= []\n",
    "#     for w in tokenizer.tokenize(t):\n",
    "#         w_lem = lemmetizer.lemmatize(w.lower())\n",
    "#         if not w_lem.isdigit() and w_lem not in stopwords.words('english'):\n",
    "#             v.append(w_lem)\n",
    "#     text_prepared.append(v)\n",
    "    \n",
    "text_prepared_fake = []\n",
    "for t in fake_news_us_el.text.values.tolist():\n",
    "    v= []\n",
    "    for w in tokenizer.tokenize(t):\n",
    "        w_lem = lemmetizer.lemmatize(w.lower())\n",
    "        if not w_lem.isdigit() and w_lem not in stopwords.words('english'):\n",
    "            v.append(w_lem)\n",
    "    text_prepared_fake.append(v)\n",
    "    \n",
    "text_prepared_true = []\n",
    "for t in true_news_us_el.text.values.tolist():\n",
    "    v= []\n",
    "    for w in tokenizer.tokenize(t):\n",
    "        w_lem = lemmetizer.lemmatize(w.lower())\n",
    "        if not w_lem.isdigit() and w_lem not in stopwords.words('english'):\n",
    "            v.append(w_lem)\n",
    "    text_prepared_true.append(v)\n",
    "\n",
    "# titles_prepared = [[lemmetizer.lemmatize(w.lower()) for w in tokenizer.tokenize(t) if w is not w.isdigit()] for t in all_data.title.values.tolist()]\n",
    "# text_prepared_fake = [[lemmetizer.lemmatize(w.lower()) for w in tokenizer.tokenize(t) if w is not w.isdigit()] for t in corpus_fake.text.values.tolist()]\n",
    "# text_prepared_true = [[lemmetizer.lemmatize(w.lower()) for w in tokenizer.tokenize(t) if w is not w.isdigit()] for t in corpus_true.text.values.tolist()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "bigrams = [list(ngrams(t,2)) for t in text_prepared_fake[:]]\n",
    "all_bigrams = set()\n",
    "\n",
    "for t in bigrams:\n",
    "    for b_gram in t:\n",
    "        all_bigrams.add(\" \".join(b_gram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1000_bigrams = list(get_most_frequent_terms(bigrams,1000).keys())\n",
    "top_2000_unigrams = list(get_most_frequent_terms(text_prepared[2:],2000).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors,not_found = compute_td_idf_fatures(top_1000_bigrams,bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_td_idf_fatures(vocabulary :list, documents :list):\n",
    "    word_to_index = {k:v for v,k in enumerate(top_1000_bigrams)}\n",
    "    not_found = []\n",
    "    td_idf_features = []\n",
    "    i = 0\n",
    "    for doc in documents:\n",
    "        vector = np.zeros(len(vocabulary))\n",
    "        #computing idf\n",
    "        idf_per_word = {}\n",
    "        j = 0\n",
    "        for w in set(doc):\n",
    "            count = 1\n",
    "            for d in documents:\n",
    "                if w in d:\n",
    "                    count+=1\n",
    "            idf_per_word[w] = np.log(len(documents)/count)\n",
    "        #computing tf\n",
    "        for w in doc:\n",
    "            if w not in word_to_index:\n",
    "                not_found.append(w)\n",
    "                continue\n",
    "            idx = word_to_index[w]\n",
    "            vector[idx] = vector[idx] + 1\n",
    "        vector = vector/len(doc)        \n",
    "        for w in doc:\n",
    "            if w not in word_to_index:\n",
    "                continue\n",
    "            idx = word_to_index[w]\n",
    "            vector[idx] = vector[idx] * idf_per_word[w]\n",
    "        td_idf_features.append(vector)\n",
    "    return td_idf_features,not_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P2PpCUaI3DVw"
   },
   "outputs": [],
   "source": [
    "def get_most_frequent_terms(terms,top_n):\n",
    "    term_to_freq = {}\n",
    "    for t in terms:\n",
    "        for w in t:\n",
    "            if w in term_to_freq:\n",
    "                term_to_freq[w] = term_to_freq[w] + 1\n",
    "            else:\n",
    "                term_to_freq[w] = 1\n",
    "     \n",
    "    res = {k: v for k, v in sorted(term_to_freq.items(), key=lambda item: item[1],reverse=True)}\n",
    "    return {k:term_to_freq[k] for k in list(res)[:top_n]}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
